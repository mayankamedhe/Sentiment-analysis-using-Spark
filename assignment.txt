In this assignment you have to process a collection of JSON files (created from news websites) to do word counts.    A zip file containing multiple input files can be downloaded from Moodle.  Each input file is a JSON file, which has several attributes; the ones we are interested in for Assignment 7A and 7B are these:  
     source_name, date_published, article_body

First unzip the input files into a directory
Follow the instructions for setting up Spark on Eclipse given on moodle.
Run the word count program which we have provided on the input files treated as text files, to make sure your Eclipse setup is proper.
Next, run both the news month count programs which find how many articles were published in each month
Today's main assignment is to modify the news month count program to find the word count in the article_body attribute by source; the output would be triples of the form (source_name, word, count).  You can either concatenate strings to get the groups, or create rows and use the .groupby() function to group on multiple attributes.

In this assignment you have to process a collection of JSON files to do sentiment analysis.    The inputs are 

A zip with JSON files, provided for part 7A containing news articles.
A list of entity names, in a file, one name per line.
Positive sentiment word and negative sentiment word lists in two files.
Your task is to do the following:

Find occurrences of entity in body; for our purpose an exact case-insensitive match between a word in the body attribute and an entity name is considered a match.
Check for nearby sentiment words, where nearby means |position-of-entity - position-of-sentiment-word| <= 5
We consider the sentiment associated with an entity occurrence as positive (+1) if a positive sentiment word is nearby, negative (-1) if a negative sentiment word is nearby; an entity occurrence could be tagged as both positive and negative.  If neither, we consider the sentiment as neutral (0). 
Your goal is to find the aggregate sentiment per entity group by (source_name, year_month, entity, sentiment_type) as follows:
NOTE: you are not allowed to use Spark SQL for this assignment
Read the sentiment word files and the entity names file into Java data structure such as HashSet<String>.  Each line of the input files has a single word.  The data structures will be automatically shared with all copies of your program when it is run in parallel.
Create dataset<row> with fields 
(source_name, year_month, entity, sentiment)  where sentiment can be -1, 0 or +1
Then get count group by (source_name, year_month, entity, sentiment)  -- raw sentiment count
  output:   (source_name, year_month, entity, sentiment, count)
from above find for each source, year_month, entity:
Overall sentiment:  sum (sentiment*count)
Overall support:     sum(count).
Now filter out all supports < 5
Output to a file (source_name, year_month, entity, overall_sentiment) tuples in decreasing order of absolute value of overall_sentiment

NOTE:  org.apache.spark.sql.functions contains aggregate functions that may be useful for this assignment. You can import this class and use aggregate methods from it by specifying the aggregates in the agg() function.  For e.g. if you have a DataSet ds1, with attributes a, b, and c, to get a result grouped by attribute 'a', with aggregates sum on attribute 'b' and average on 'c' per group 

          ds1.groupBy("a").agg(functions.sum("b"),functions.avg("c"));

But if you want only a single aggregate, it's simpler to use
          ds1.groupBy("a").sum("b");

You can also use ds1.col("a") instead of directly giving the column name.

Documentation of functions class is available at https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/functions.html

